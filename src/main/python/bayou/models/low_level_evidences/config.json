{
    "model": "lle",
    "latent_size": 64,
    "batch_size": 50,
    "num_epochs": 100,
    "learning_rate": 0.0006,
    "print_step": 50,
    "checkpoint_step": 5,
    "evidence": [
        {
            "name": "apicalls",
            "units": 128,
            "num_layers": 1,
            "tile": 1,
            "max_depth": -1,
            "max_nums": 8,
            "ev_drop_prob":1.0,
            "ev_call_drop_prob":1.0
        },
        {
            "name": "types",
            "units": 128,
            "num_layers": 1,
            "tile": 1,
            "max_depth": -1,
            "max_nums": 8,
            "ev_drop_prob":1.0,
            "ev_call_drop_prob":1.0
        },
        {
            "name": "keywords",
            "units": 128,
            "num_layers": 1,
            "tile": 1,
            "max_depth": -1,
            "max_nums": 8,
            "ev_drop_prob":1.0,
            "ev_call_drop_prob":1.0
        },
        {
            "name": "callsequences",
            "units": 128,
            "num_layers": 1,
            "tile": 1,
            "max_depth": 8,
            "max_nums": 1,
            "ev_drop_prob":1.0,
            "ev_call_drop_prob":1.0
        },
        {
            "name": "returntype",
            "units": 256,
            "num_layers": 1,
            "tile": 1,
            "max_depth": -1,
            "max_nums": 1,
            "ev_drop_prob":1.0,
            "ev_call_drop_prob":1.00
        },
        {
            "name": "formalparam",
            "units": 256,
            "num_layers": 1,
            "tile": 1,
            "max_depth": 8,
            "max_nums": 1,
            "ev_drop_prob":1.0,
            "ev_call_drop_prob":1.0
        }
    ],
    "decoder": {
        "units": 512,
        "num_layers": 1,
        "max_ast_depth": 32
    },
    "reverse_encoder": {
        "units": 512,
        "num_layers": 1,
        "max_ast_depth": 32
    }
}
